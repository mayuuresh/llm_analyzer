from openai import OpenAI
from google import genai

# Initialize OpenAI client for GPT-4
gpt4_client = OpenAI(
    api_key="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"
)

# Initialize OpenRouter clients
openrouter_client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="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"
)

# Initialize Gemini
gemini_client = genai.Client(api_key="AIzaSyCE9EGd7AIPfq9JQlBAfV7H4axNZ5eZTuA")

def get_gpt4_response(prompt):
    try:
        completion = gpt4_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}]
        )
        return completion.choices[0].message.content
    except Exception as e:
        return f"GPT-4 Error: {str(e)}"

def get_llama_response(prompt):
    try:
        completion = openrouter_client.chat.completions.create(
            model="meta-llama/llama-3.3-8b-instruct:free",
            messages=[{"role": "user", "content": prompt}]
        )
        return completion.choices[0].message.content
    except Exception as e:
        return f"Llama Error: {str(e)}"

def get_mistral_response(prompt):
    try:
        completion = openrouter_client.chat.completions.create(
            model="mistralai/mistral-nemo:free",
            messages=[{"role": "user", "content": prompt}]
        )
        return completion.choices[0].message.content
    except Exception as e:
        return f"Mistral Error: {str(e)}"

def get_deepseek_response(prompt):
    try:
        completion = openrouter_client.chat.completions.create(
            model="deepseek/deepseek-chat-v3-0324:free",
            messages=[{"role": "user", "content": prompt}]
        )
        return completion.choices[0].message.content
    except Exception as e:
        return f"DeepSeek Error: {str(e)}"

def get_gemini_response(prompt):
    try:
        response = gemini_client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt
        )
        return response.text
    except Exception as e:
        return f"Gemini Error: {str(e)}"

def evaluate_responses_with_gpt4(responses):
    evaluation_prompt = "\n".join(
        [f"Response from {model}: {response}" for model, response in responses.items()]
    )
    evaluation_prompt = (
        "Evaluate the following responses and determine which is the most accurate and fulfills the user's intent. "
        "Provide only the content of the chosen response and the name of the model in a concise format.\n" + evaluation_prompt
    )

    try:
        completion = gpt4_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": evaluation_prompt}]
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        return f"Evaluation Error: {str(e)}"

def main():
    while True:
        user_prompt = input("\nEnter your prompt (or 'quit' to exit): ")
        
        if user_prompt.lower() == 'quit':
            break

        print("\nGetting responses from all models...")
        
        # Get responses from all models
        responses = {
            "GPT-4": get_gpt4_response(user_prompt),
            "Llama": get_llama_response(user_prompt),
            "Mistral": get_mistral_response(user_prompt),
            "DeepSeek": get_deepseek_response(user_prompt),
            "Gemini": get_gemini_response(user_prompt)
        }

        # Evaluate responses using GPT-4
        best_response = evaluate_responses_with_gpt4(responses)

        # Display the chosen response and model
        print("\nFinal Output:")
        print(best_response)

if __name__ == "__main__":
    main()
